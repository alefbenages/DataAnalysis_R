---
title: "Multiple_Regression"
author: "Ale F. Benages"
date: "4/19/2021"
output: html_document
---

# Intro
You have been asked to predict the sales in four different product types while assessing the effects service and customer reviews have on sales. You'll be using Regression to build machine learning models for this analyses. Once you have determined which one works better on the provided data set, you will be asked to predict the sales of four product types from the new products list and prepare a report of your findings.

> Output Var: Volume



# libraries
```{r}
library(skimr) # to get a fast overview of all the variables. Fast&Dirty EDA
library(caret, ggplot2)
library(fastDummies)
library(plyr)
library(corrplot) # to create a heatmap with correlations
```


# Load CSV - ExsProds
```{r}
path_to_file <- "//home/ale/Dropbox/UBIQUM/3.DAwithR/Task3:MultipleRegressioninR/productattributes/existingproductattributes2017.csv"

ExsProds <- read.csv(path_to_file)

str(ExsProds) # with srt() we can see the structure of the dataframe
head(ExsProds) # with head we get a similar output but with the classic dataframe structure
```
# EDA 
```{r}
skim(ExsProds)
```

## Datatypes 

Also change the datatypes of the categorical variable from char to factor, and add labels. 
```{r}
# change the datatype to factor 
ExsProds[, 'ProductType'] <- as.factor(ExsProds[, 'ProductType'])

#add the corresponding lables
# ProductType_labels = c("Accesories", "Display", "ExtendedWarranty", "GameConsole", " Laptop", "Netbook", "PC", "Printer", "Printer Supplies", "Smartphone", "Software", "Tablet")
# ExsProds$ProductType <- plyr::mapvalues(ExsProds$ProductType, from=c(1:12), to=ProductType_labels)

# str(ExsProds)
```

### Type of variables  

**Numerical Variables**  

- Price
- ShippingWeight, ProductDepth, ProductWidth, ProductHeight     
- ProfitMargin
- Volume
- x5StarReviews, x4StarReviews, x3StarReviews, x2StarReviews, x1StarReviews
- PositiveServiceReview, NegativeServiceReview 

> **xXStarReviews** and **Pos/NegServiceReview**, if condensed in just one variable, would been Categorical Variables, but splited like that into their inner values, they are cosidered **numerical variables**, because the category is alredy defined in the name. 

**Ordinal Categorical Variables**  

- ProductNum
- BestSellersRank

**Nominal Categorical Variables** 

- ProductType
 
## Pre-Processing

### dummyfy the categorical variable
Categorical variables may be used directly as predictor or predicted variables in a multiple regression model as long as they've been converted to binary values. In order to pre-process the sales data as needed we first need to convert all factor or 'chr' classes to binary features that contain ‘0’ and ‘1’ classes.

```{r}
ExsProds <- fastDummies::dummy_cols(ExsProds)  #library::function
str(ExsProds)
```
> the ProductType feature now is dummified, or splited into its inner values. 
> Notice that this feature is "duplicated": the original list, and the new features. 
> Notice also that now it became a **bigger** dataset, 29 features x 80 cols. 



### Handling NAN's

The skim function told that there are some NAN's in the _BestSellersRank_ feature. 

There are many methods of addressing missing data, but the most secure is simply deleting any attribute that has missing information. In any attempt of fixing those NA or NAN's, the integrity of the dataset will be affected. 
Only in cases where the dataset is very constrained and getting new data is difficult/expensive/inviable has sense to invest time and effort to replace the missing data with guessed data.

```{r}
ExsProds <- na.omit(ExsProds) # Using na.omit() to remove (missing) NA and NaN values

any(is.na(ExsProds)) # check if there is any NA, NAN or missing values
```

### Drop Non Numerical data
```{r}
ExsProds_Clean <- ExsProds #creates a copy of the DF

ExsProds$ProductType <- NULL 
```


### Reorder the columns
move the output feature to the end of the dataset, just to improve visualization.  
```{r}
ExsProds[ , c("ProductNum", "Price", "x5StarReviews", "x4StarReviews", "x3StarReviews", "x2StarReviews", "x1StarReviews", "PositiveServiceReview", "NegativeServiceReview", "Recommendproduct", "BestSellersRank", "ShippingWeight", "ProductDepth", "ProductWidth", "ProductHeight", "ProfitMargin", "ProductType_Accessories", "ProductType_Display", "ProductType_ExtendedWarranty", "ProductType_GameConsole", "ProductType_Laptop", "ProductType_Netbook", "ProductType_PC", "ProductType_Printer", "ProductType_PrinterSupplies", "ProductType_Smartphone", "ProductType_Software", "ProductType_Tablet", "Volume")]   # Reorder columns by names
```

## Some Plots

```{r}
Numerical_Features <- colnames(ExsProds)

for (i in Numerical_Features){
  hist(ExsProds[,i], main = paste("Histogram of ",i), xlab = i)
}
```


## Correlation
While correlation doesn't always imply causation you can start your analysis by finding the correlation between the relevant independent variables and the dependent variable. 

The cor() function to creates a correlation matrix that you can visualize to ascertain the correlation between all of the features.
```{r}
correlationMatrix <- cor(ExsProds)

# uncomment to visualize the matrix
# corrData
```

As the corrMatrix is very big and difficult to visualize, it's better to use a graph.
```{r}
# Adjust Margins
#par(mar=c(3,1,5,1)) #bottom, left, top, right respectively.

png(height=1200, width=1200, pointsize=25, file="overlap.png")

corrplot(correlationMatrix
)

```

![]("//home/ale/Dropbox/UBIQUM/3.DAwithR/Task3:MultipleRegressioninR/productattributes/overlap.png")

The corr parts are:
- The high correlation between volume and high rated reviews. -> well rated prducts have a high volume of sells 
-  Something similar happends with Positive/Negative Service Review. Positive are more correlated with Volume. 
- the highest correlated product with volume are the game consoles.  A positive correlation.
- and a negative correlation can be founded with the weight. Heavy products have a lower sells' volume.
- price is also somehow negative correlated...(edited)

other interesting things, not related with volume, is that the product with higher profit margin are the extended warranty. And accessories the least.

But to remove columns, I prefer using varImp()



# Regression

## Train/Test Split
```{r}
# define an 75%/25% train/test split of the dataset
inTraining <- caret::createDataPartition(ExsProds$Volume, p = .75, list = FALSE)
training <- ExsProds[inTraining,]
testing <- ExsProds[-inTraining,]
```

## Models
### Multiple linear regression model
```{r}
model_mlr <- train(Volume ~ .,
               data = training ,
               method = "lm")

model_mlr

```

```{r}
predictions_mlr <- predict(model_mlr, testing)

```



### Ridge regression model
```{r}
model_Rrm <- train(Volume ~ .,
               data = training,
               method = "ridge") # Try using "lasso"

model_Rrm

```




# DUDAS
- Normalization... how to handle the product number ot the dummy productTypes ?
- 
## Normalization

```{r}
# #define Min-Max normalization function
# min_max_norm <- function(x) { 
#   (x - min(x)) / (max(x) - min(x))
#   }
```

```{r}
# #apply Min-Max normalization to the numerical variables
# ExsProds <- as.data.frame(lapply(colnames(ExsProds)(2:)], min_max_norm))
# #print structure
# str(ComResp_norm)
# #summary
# summary(ComResp_norm)
```











# Load CSV - new_Products

```{r}
path_to_file <- "//home/ale/Dropbox/UBIQUM/3.DAwithR/Task3:MultipleRegressioninR/productattributes/newproductattributes2017.csv"

NewProds <- read.csv(path_to_file)

str(NewProds) # with srt() we can see the structure of the dataframe
head(NewProds) # with head we get a similar output but with the classic dataframe structure
```



