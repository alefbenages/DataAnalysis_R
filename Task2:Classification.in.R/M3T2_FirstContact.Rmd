---
title: "M3T2_FirstContact"
author: "Ale F. Benages"
date: "3/30/2021"
output: html_document
---

# Library
```{r}
library(caret, ggplot2)
library(ggridges)
library(plyr)
library(skimr) # to get a fast overview of all the variables. Fast&Dirty EDA
#library(tictoc)
library(mlbench)
library(gbm)
set.seed(9982)
```


# Load CSV
```{r}
path_to_file <- "//home/ale/Dropbox/UBIQUM/3.DAwithR/Task2:Classification.in.R/SurveyData/CompleteResponses.csv"

ComResp <- read.csv(path_to_file)

str(ComResp) # with srt() we can see the structure of the dataframe
# head(ComResp) # with head we get a similar output but with the classic dataframe structure
```

# EDA 
**Numerical Variables**  
* Salary  
* Age  
* Credit
**Ordinal Categorical Variables**  
* elevel
**Nominal Categorical Variables**  
* Zipcode  
* Car  
* Brand

## Datatypes
Datatypes from the categorical (ordinal or nominal) variables are incorrect: **elevel, car, zipcode and brand** datatypes should be changed to factor. 
```{r}
# change the datatype to factor 
ComResp[, 'elevel'] <- as.factor(ComResp[, 'elevel'])
ComResp[, 'car'] <- as.factor(ComResp[, 'car'])
ComResp[, 'zipcode'] <- as.factor(ComResp[, 'zipcode'])
ComResp[, 'brand'] <- as.factor(ComResp[, 'brand'])

str(ComResp)
```

### add labels
also we can add the corresponding labels to each categorical variable
```{r}
# list of labels of elevel
ed_level_labels = c("Less than High School Degree", "High School Degree", "Some College","4-Year College Degree",
             "Master's, Doctoral or Professional Degree")
# create a new column 'elevelLabeled', copy of elevel with values remaped
ComResp$elevelLabeled <- mapvalues(ComResp$elevel, from=c(0:4), to=ed_level_labels)

cars_labels = c("BMW", "Buick", "Cadillac", "Chevrolet", "Chrysler", "Dodge", "Ford", "Honda", "Hyundai", "Jeep", "Kia", "Lincoln", "Mazda", "Mercedes Benz", "Mitsubishi", "Nissan", "Ram", "Subaru", "Toyota", "None of the above")
ComResp$carLabeled <- mapvalues(ComResp$car, from=c(0:19), to=cars_labels)

zipcode_labels = c("New England", "Mid-Atlantic", "East North Central", "West North Central", "South Atlantic", "East South Central", "West South Central", "Mountain", "Pacific")
ComResp$zipcodeLabeled <- mapvalues(ComResp$zipcode, from=c(0:8), to=zipcode_labels)

brands_labels = c("Acer", "Sony")
ComResp$brandLabeled <- mapvalues(ComResp$brand, from=c(0:1), to=brands_labels)
```

the Skim function gives us a great overview of the dataset
```{r}
skim(ComResp)
```

## Missing values and NA ?
* No missing values


## Plots of each variables. 

### 1. Salary 

Survey Questions and Response Key  
1) What is your yearly salary, not including bonuses?. Respondents enter numeric value

* **This is a numerical variable. **

```{r}
summary(ComResp$salary)
```

```{r fig.height=2.5, fig.width=6}
ggplot( ComResp, aes(x=salary)) + theme_bw()+
  geom_histogram(color = 'black', fill = '#FF6620', binwidth = 10000)+
  labs(title = "Salary Distribution", x="Salary") + 
  geom_vline(aes(xintercept=mean(salary)), color="blue", linetype="dashed", size=1)  # adds the mean
```

### 2. Age

Survey Questions and Response Key  
2) What is your age?. Respondents enter numeric value

* ** This is a Numerical Variable**

```{r}
summary(ComResp$age)
```

```{r fig.height=2.5, fig.width=6}
ggplot( ComResp, aes(x=age)) + theme_bw()+
  geom_histogram(color = 'black', fill = 'yellow', binwidth = 10)+
  labs(title = "Age Distribution", x="Age") +
  geom_vline(aes(xintercept=mean(age)), color="blue", linetype="dashed", size=1)  # adds the mean
```
> in general the customers have 30 to 70 years old. 

### 3. elevel - Education Level 

3) What is the highest level of education you have obtained?. Respondents select from the following 5 choices:  
Value : Description  
0 : Less than High School Degree  
1	: High School Degree  
2 : Some College  
3 : 4-Year College Degree  
4 : Master's, Doctoral or Professional Degree  

* **This is a categorical ordinal variable**

```{r}
# print the head just to verify everything is correct
head(ComResp)
```

```{r}
# magins of the plot
# mar: A numerical vector of the form c(bottom, left, top, right) which gives 
# the number of lines of margin to be specified on the four sides of the plot. 
# The default is c(5, 4, 4, 2) + 0.1.
par(mar=c(3.5,10.5,3,1)+0.1)

#TODO why im using a table here? 
counts <- table(ComResp$elevelLabeled)
# finally a Simple Horizontal Bar Plot with Added Labels
barplot(counts, main="Education Levels Distribution", horiz=TRUE, names.arg=ed_level_labels, cex.names=0.6, las=2)
```
> In general the education level of customers is very homogeneous. 


### 4. Car 

4) What is the make of your primary car?. Respondents select from the following 20 choices:	
Value : Description
1 : BMW  
2 : Buick  
3 : Cadillac  
4 : Chevrolet  
5 : Chrysler  
6 : Dodge  
7 : Ford  
8	: Honda  
9	: Hyundai  
10	: Jeep  
11	: Kia  
12	: Lincoln  
13	: Mazda  
14	: Mercedes Benz  
15	: Mitsubishi  
16	: Nissan  
17	: Ram  
18	: Subaru  
19	: Toyota  
20	: None of the above  

* **this is a Nominal categorical variable**


```{r}
# Simple Horizontal Bar Plot with Added Labels
par(mar=c(3,7.5,3,1)+0.1)
# mar: A numerical vector of the form c(bottom, left, top, right) which gives 
# the number of lines of margin to be specified on the four sides of the plot. 
# The default is c(5, 4, 4, 2) + 0.1.

Car_counts <- table(ComResp$carLabeled)

barplot(Car_counts, main="Car's Brands Distribution", horiz=TRUE, names.arg=cars_labels, cex.names=0.6, las=1,)
```

> There is no preffered car's brand. the chocices are very homogeneous. 



### 5. zipcode

5) What is your zip code?. Respondents enter zip code, which is captured as 1 of the following 9 regions in the U.S.  
Value :	Region  
0 :	New England  
1 :	Mid-Atlantic  
2 :	East North Central  
3 :	West North Central  
4 :	South Atlantic  
5 :	East South Central  
6 :	West South Central  
7 :	Mountain  
8 :	Pacific  

* **This is a categorical Nominal variable**

```{r fig.height=5, fig.width=6}
# Simple Horizontal Bar Plot with Added Labels
par(mar=c(3,7.5,3,1)+0.1)
# mar: A numerical vector of the form c(bottom, left, top, right) which gives 
# the number of lines of margin to be specified on the four sides of the plot. 
# The default is c(5, 4, 4, 2) + 0.1.

zipcode_counts <- table(ComResp$zipcodeLabeled)

barplot(zipcode_counts, main="ZipCode's Distribution", horiz=TRUE, names.arg=zipcode_labels, cex.names=0.6, las=1,)
```

### 6. Credit

6) What amount of credit is available to you?. Respondents enter numeric value.

* **Quantitative variable** 

```{r}
summary(ComResp$credit)
```

```{r  fig.height=2.5, fig.width=6}
ggplot( ComResp, aes(x=credit)) + theme_bw()+
  geom_histogram(color = 'black', fill = '#FF6620', binwidth = 10000)+
  labs(title = "Credit Distribution", x="Salary") + 
  geom_vline(aes(xintercept=mean(credit)), color="blue", linetype="dashed", size=1)  # this line adds the mean
```

### 7. brand

7) Which brand of computers do you prefer?. Respondents select from the following 2 choices:  
Value : Description  
0	: Acer  
1	: Sony  

> this is our **Target variable.**

```{r}
brand_table <- table(ComResp$brand)

barplot(brand_table, 
        col = rainbow(2), 
        names.arg = c("Acer", "Sony"))
```


## Normalization

**

The formula for a min-max normalization is:
$$(X – min(X))/(max(X) – min(X))$$
For each value of a variable, we simply find how far that value is from the minimum value, then divide by the range. To implement this in R, we can define a simple function and then use lapply to apply that function to whichever columns in the dataset:

```{r}
#define Min-Max normalization function
min_max_norm <- function(x) { 
  (x - min(x)) / (max(x) - min(x))
  }
```

```{r}
#apply Min-Max normalization to the numerical variables
ComResp_norm <- as.data.frame(lapply(ComResp[c('salary','age','credit')], min_max_norm))
str(ComResp_norm)
summary(ComResp_norm)
```

The result looks great, all the numerical variables from 0 to 1, and also they are very symmetric. 1Q is close to 0.25, Mean ~ 0.5 and 3Q ~ 0.75.  
But notice that all the rest of the variables were dropped out. They must be added manually. 
```{r}
ComResp_norm$elevel <- ComResp$elevel 
ComResp_norm$car <- ComResp$car
ComResp_norm$zipcode <- ComResp$zipcode
ComResp_norm$brand <- ComResp$brand

ComResp_norm$elevelLabeled <- ComResp$elevelLabeled 
ComResp_norm$carLabeled <- ComResp$carLabeled
ComResp_norm$zipcodeLabeled <- ComResp$zipcodeLabeled
ComResp_norm$brandLabeled <- ComResp$brandLabeled

str(ComResp_norm)
```


## Feature Selection   
[Feature Selection on R](https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/)  
Selecting the right features in your data can mean the difference between mediocre performance with long training times and great performance with short training times. 

```{r}

# Reorder columns by names.
#ComResp_norm[, c("brand","salary","age","elevel","car","zipcode","credit","brandLabeled","elevelLabeled","carLabeled","zipcodeLabeled")] 




#str(ComResp) # print the head of the full dataset
#ComResp_norm_numeric <- ComResp_norm[0:7]   #select only the columns with numeric data
ComResp_norm_numeric <- ComResp_norm[c("brand","salary","age","elevel","car","zipcode","credit")]   #select only the columns with numeric data

str(ComResp_norm_numeric)
```


### Correlation Analisis
Data can contain attributes that are highly correlated with each other. Many methods perform better if highly correlated attributes are removed.

The Caret R package provides the findCorrelation which will analyze a correlation matrix of your data’s attributes report on attributes that can be removed. A correlation matrix is created from these attributes and highly correlated attributes are identified. Generally, you want to remove attributes with an absolute correlation of 0.75 or higher.

There is a 



```{r}
options(scipen = 999)    # removes the scientific notation 

# calculate correlation matrix
correlationMatrix <- cor(ComResp_norm_numeric)
print(correlationMatrix)
```

we can see that the highest correlation value we got for the Brand Variable (ignoring the correlation with itself) is:     
* Salary: 0.2     
* Age: 0.13     
The rest of the variables have very low impact on the brand choice. 

**Remember that the correlation could be positive or negative (from -1 to +1), so we look for the absolute value.** 


Also we can use the function findCorrelation() to look for us for the highly correlated variables. In this case it's worthless (because there are very few variables) but it's an interesting tool. More info th this tool [here](https://rdrr.io/cran/caret/man/findCorrelation.html) 

```{r}
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(cor(ComResp_numeric), cutoff=0.15, names=TRUE)
# print indexes of highly correlated attributes
print(highlyCorrelated)
```
To interpret the result we must know that is taking the highest correlated pairs from the first column (this is the [1]).

Another method is using the corrplot [+ info](https://www.rdocumentation.org/packages/corrplot/versions/0.84/topics/corrplot) 
```{r}
# load library
library(corrplot)

# create correlation plot
corrplot(correlationMatrix, method="pie", diag=TRUE)
```                                           

Finally, a visual check can be made by plotting the boxplots of all the variables divided by their brands. As more different is the two brand choice the better. 
```{r}
boxplot(salary ~ brandLabeled, data = ComResp, col = rainbow(2), names.arg = c("Acer", "Sony") )
boxplot(age ~ brandLabeled, data = ComResp, col = rainbow(2), names.arg = c("Acer", "Sony") )
boxplot(elevel ~ brandLabeled, data = ComResp, col = rainbow(2), names.arg = c("Acer", "Sony") )
boxplot(car ~ brandLabeled, data = ComResp, col = rainbow(2), names.arg = c("Acer", "Sony") )
boxplot(zipcode ~ brandLabeled, data = ComResp, col = rainbow(2), names.arg = c("Acer", "Sony") )
boxplot(credit ~ brandLabeled, data = ComResp, col = rainbow(2), names.arg = c("Acer", "Sony") )
```
That confirm what the results obtained. 

### Remove redundant Data
So, after the correlation analysis we can create a copy of the dataset with just the most related variables.  
* Input Variables:  Salary, Age
* Output Variable:  Brand


```{r}
ComResp_reduced <- ComResp[, c("salary","age","brandLabeled")]
str(ComResp_reduced)
```


### Some extra notes about Correlation and p-values
For **Correlation** the **p-Value**: tell us the probability  that randomly drawn dots will result in a similarly strong relationship, or stronger. The **smaller the p-Value, the more confidence we have in the predictions we make with the line**. 
If we have 2 lines with a similar correlation value, we would prefer the one with the smaller p-Value. 
![](/home/ale/Dropbox/UBIQUM/3.DAwithR/Task2:Classification.in.R/M3T2_FirstContact_files)

When correlation = 0, a value on the x-axis doesn't tell us anything about what to expect on the y-axis.. because there is no reason to choose one value over another.  
As long as the correlation value is not 0  we can still make some inferences, but our guesses become more refines the closer the correlation values get to -1 or 1. And the confidence in our inferences depends on the amount of data we have collected and the p-Value. 

# Train the Model


## Train/Test Split


```{r}
# define an 75%/25% train/test split of the dataset
inTraining <- createDataPartition(ComResp_reduced$brandLabeled, p = .75, list = FALSE)
training <- ComResp_reduced[inTraining,]
testing <- ComResp_reduced[-inTraining,]
```


## Stochastic Gradient boosting ()
```{r}
fitControl_gbmFit1 <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10)
```

```{r}
gbmFit1 <- train(brandLabeled ~ ., ## define the input formula as "y~x1,x2...xn" OR "y~." for short here, "~" can be read as "is defined as", while the "." indicates all variablesin the dataframe
                 data = training, 
                 method = "gbm", 
                 trControl = fitControl_gbmFit1,
                 ## This last option is actually one for gbm() that passes through
                 verbose = FALSE)
gbmFit1
```

```{r}
trellis.par.set(caretTheme())
plot(gbmFit1)  
plot(gbmFit1, metric = "Kappa")
```

## Random Forest
```{r}
#10 fold cross validation
fitControl_rf1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

#train Random Forest Regression model with a tuneLenght = 5 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brandLabeled ~., 
                data = training, 
                method = "rf", 
                trControl=fitControl_rf1, 
                tuneLength = 5)

rfFit1 
```

```{r}
plot(rfFit1)  
```

                                                            
```{r}
#10 fold cross validation
fitControl_rf2 <- trainControl(method = "repeatedcv", number = 10, repeats = 1,search = 'random' )

#train Random Forest Regression model with a tuneLenght = 5 (trains with 1 mtry value for RandomForest)
rfFit2 <- train(brandLabeled ~., 
                data = training, 
                method = "rf", 
                trControl=fitControl_rf2)

rfFit2 
```

```{r}
plot(rfFit2)
```


# DUDAS
    
- como 'uso/interpre' estos resultados?.
    
- #TODO remember to check collinearity.
      
- normalize
      
- Kappa. 
    if unbalanced data, take care of kappa. 


```{r}
- #TODO remember to check collinearity.
      
```









